# -*- coding: utf-8 -*-
"""(4)LinearRegression_StochasticGradientDescent-Worldbank_USLifeExpectancyatBirth.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wbhd8hXm8tC1PP1l3TOs4W336uGNZYgY

## **MLPP25 // Linear Regression as Machine Learning: Life Expectancy Over Time**

*Feb 13, 2025*

This notebook will go through an example of Linear Regression in a Machine Learning context using publicly available USA life expectancy at birth data.

---
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px

# -- set matplotlib defaults, all style sheets: https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html

# pandas use matplotlib in the backend
# all the plotting functions in pandas are like wrappers around matplotlib.
# so we can set a style sheet for matplotlib and it will apply to pandas plotting
# we can use the above link to get access to all the types of interfaces/ background/ styles that can be used

#matplotlib.pyplot - pyplot is like the submodule within matplotlib.
plt.style.use("bmh")

# -- define plotly express defaults
px.defaults.width = 900
px.defaults.template="ggplot2"

# -- set the filename and read in the data
fname = "/content/drive/Shareddrives/mlpp25/data/worldbank/wb_us_leb.csv"
wbdat = pd.read_csv(fname)
wbdat

# -- plot the World Bank data
ax = wbdat.plot("year", "leb_us", kind="scatter", xlabel="birth year",
                ylabel="US life expectance at birth [yr]", #dont forget to incldue the units
                color="steelblue", s=40, figsize=(7, 4))

#This is the dot plot
#x-axis - year
#y-axis - life expectancy at birth
#scatter plot
#xlabel and ylabel given to the axes
#s=40 changing the size of the dots in the scatter plot
#figsize - changing the size of the figure.

"""Inferences:
- upward trend
- but not monotonously increasing, but there is upward trend in general
- now, we can choose the model to be applied - we can use linear model here, which is concluded based on the scatter plot.

- we need to plot the data to watch the trend, so that we can understand what model to be used to fit.

we want to fit a straight line to the data using *Stochastic Gradient
Descent*, so let's import it from `scikit-learn` (abbreviated as sklearn), #refer the scikit-learn website
"""

# -- import scikit-learn's Stochastic Gradient Descent regressor
from sklearn.linear_model import SGDRegressor

#SGDRegressors are like the ones that implement stochastic gradient descent.

# -- now, we need to initialize an SGD regression model
lm = SGDRegressor()

# its like creating an instance
# analogy == SGDRegressor:human :: lm:person1
# it can be considered an object. object has attributes and methods. attributes are values associated with the object and methods are the functions associated with the object.

# -- fit the model with the data
lm.fit(wbdat[["year"]], wbdat[["leb_us"]])

# (wbdat["year"]) - gives a column
# type(wbdat["year"]) - series
# type(wbdat[["year"]]) - dataframe of 1 column
# type(wbdat[["year", "leb_us"]]) - dataframe of 2 columns.

"""For `sklearn`, independent variables must be in a 2D shape with each independent variable in a column (we'll see why later).  Now let's use the fit to make the model "prediction" for each year,"""

lm.coef_
lm.intercept_
#the attributes with underscores - attributes that have been modified by the fitting process.

# -- "predict" the data
model = lm.predict(wbdat[["year"]]) #predict takes the x-values (year) and predicts the y values.

# -- add prediction back to the original DataFrame - wbdat
wbdat["model"] = model

# -- plot the data
ax = wbdat.plot("year", "leb_us", kind="scatter", xlabel="birth year",
                ylabel="US life expectance at birth [yr]",
                color="steelblue", label="WB data", s=40, figsize=(7, 4))

# -- add the model line to the plot
ax = wbdat.plot("year", "model", label="linear model", color="red", ax=ax) #ax=ax is like putting this line onto the plot above.

"""The raw model fit looks pretty bad...  This is because of a mismatch between the scale of the x and y data and the very low alpha parameter in the model.  We need to **"*standardize*"** the data to put the dimensions on equal footing.

standardizing the data
- rather than having your data like between 70 and 78, we can have it between -2 and 2. same with the birth year too.
- we want to transform our data into values that preserve the relationships between the columns, but have the values on more equal footing.


The formula to standardize each value is: ùëß=(ùë•‚àíùúá)/ùúé

Where:

ùëß = standardized value (also called the z-score)

ùë• = original value

ùúá = mean of the column

ùúé = standard deviation of the column
"""

# -- standardize the data
wbdat["year_st"] = (wbdat["year"] - wbdat["year"].mean()) / wbdat["year"].std()
wbdat["leb_us_st"] = (wbdat["leb_us"] - wbdat["leb_us"].mean()) / wbdat["leb_us"].std()

# -- let's plot the standardized data to see how it worked (notice the limits of the axes now)
ax = wbdat.plot("year_st", "leb_us_st", kind="scatter",
                xlabel="birth year [standardized]",
                ylabel="US life expectance at birth [standardized]",
                color="steelblue", label="WB data", s=40, figsize=(7, 4))

# -- redefine the the SGD regression model to use with the standardized data
lm_st = SGDRegressor() #new model now

# -- fit the standardized data
lm_st.fit(wbdat[["year_st"]], wbdat[["leb_us_st"]])

# -- "predict" the standardized data
model_st = lm_st.predict(wbdat[["year_st"]])
wbdat["model_st"] = model_st #predicted y values - standardised

# -- plot the standardized data
ax = wbdat.plot("year_st", "leb_us_st", kind="scatter",
                xlabel="birth year [standardized]",
                ylabel="US life expectance at birth [standardized]",
                color="steelblue", label="WB data", s=40, figsize=(7, 4))

# -- add the model
ax = wbdat.plot("year_st", "model_st", label="linear model", color="red", ax=ax)

"""THe relationship is built well and the linear model is fit, but we cannot take this graph to a policy maker as the values are not in the exact units (like, years are between -1.5 to 1.5). So we need to unstandardise the model.

Let's "un-standardize" the model so that it can be compared to the raw data,

To unstandardize a value (i.e., convert a standardized z-score back to the original value), you use the inverse of the standardization formula:

ùë• = (ùëß*√ó)+ùúé

Where:

ùë• = original value (what we‚Äôre calculating)

ùëß = standardized value

ùúá = mean of the original column

ùúé = standard deviation of the original column
"""

# -- "un-standardize" the model
model = wbdat["leb_us"].std() * wbdat["model_st"] + wbdat["leb_us"].mean()
wbdat["model"] = model

# -- plot the final model (with plotly)

# creating labels disctionary
labs={
    "leb_us":"US Life Expectancy at birth [yr]",
    "model":"Linear Model"

}

fig=px.scatter(wbdat, "year", "leb_us",
               color_discrete_sequence= ["steelblue"],
               labels=labs)
fig.update_traces(showlegend=True, name="World Bank Data")

# make a line plot of the model
figline=px.line(wbdat, "year", "model", labels=labs)
fig.update_traces(showlegend=True, name="Linear Model")

# add linear model to the scatter plot
fig.add_traces(figline.data)

fig.show()
# figline.show()

"""Let's calculate the mean squared error, MSE $ = \sum (data - model)^2 / N_{data}$,"""

# -- calculate MSE (Mean Squared Error)
mse = np.mean((wbdat["leb_us"]-wbdat["model"])**2)
print("MSE = {0:0.3}".format(mse))

"""The most common way to use MSE is to compare it to the variance in the raw data:"""

# -- calculate the ratio of MSE to variance
# how big are the errors relative to the actual spread of the data
var = np.var(wbdat["leb_us"])
print("MSE/var = {0:0.3}".format(mse / var))

#the error is about 3% of the variance of the data.

"""Another common measure of goodness of fit (that also doesn't take into account uncertainty in a measurement) is $R^2$ or the *fraction of variance in the data that is explained by the model*,"""

# -- calculate R^2
r2 = 1 - np.var(wbdat["leb_us"] - wbdat["model"]) / np.var(wbdat["leb_us"])
print("R^2 = {0:0.2}".format(r2))

# -- import scikit-learn's R^2 function
from sklearn.metrics import r2_score

# -- calculate R^2 using sklearn
r2 = r2_score(wbdat["leb_us"], wbdat["model"])
print("R^2 = {0:0.2}".format(r2))

"""Mean Squared Error (MSE) and R-squared (R¬≤) are both regression evaluation metrics but serve different purposes. MSE measures the average of the squared differences between predicted and actual values, giving a sense of how far off predictions are‚Äîlower MSE indicates better performance, though it's sensitive to outliers and is in squared units of the target. In contrast, R¬≤ indicates the proportion of variance in the dependent variable that is explained by the model, ranging from negative values (poor models) to 1 (perfect fit); it's unitless and provides an intuitive measure of how well the model captures the data‚Äôs variability.

"""